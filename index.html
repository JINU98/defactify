<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Defactify 4 Workshop</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<!-- Link to external CSS file -->
    <link rel="stylesheet" href="style.css">
</head>

<body class="is-preload">

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<div id="main">
			<div class="inner">

				<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo"><strong>Defactify 4</strong> Workshop</a>
					

					<a href="ai_gen_txt_detection.html" class="task_links task_link1" style="margin: 0 20px 0 0;">
					
					<!-- CT2: AI-Generated Text Detection -->
					<span class="label" style="font-size: small;">CT2: AI-Generated Text Detection</span>
					</a>
					<a href="ai_gen_img_detection.html" class="task_links task_link2"><span class="label">CT2: AI-Generated Image Detection</span></a>
				</header>
	
				<!-- Content -->
				<section>

					<div style="background: url('https://aiisc.ai/defactify/img/background.gif'); border-radius: 2rem; padding-bottom: 2rem;">
						<header class="main" style="text-align: center;">
							<span class="logo_image main"><img src="./images/factifY_4.png"  style="width:90%;"alt="" /></span>

							<div>
								<h1 class="heading_bannerh1" style="color: #fff;"><a href="https://aaai.org/aaai-conference/" target="_blank"
										rel="noopener noreferrer">@ AAAI 2025</a> </h1>
								<h2 class="heading_bannerh2" style="color: #fff;">Fourth Workshop on ​Multimodal Fact Checking and Hate Speech
									Detection <br>
									February, 2025</h2>
							</div>

						</header>
					</div>

					<!-- <p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat. Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris. Curabitur sapien risus, commodo eget turpis at, elementum convallis elit. Pellentesque enim turpis, hendrerit.</p>
									<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum. Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum. Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus. Pellentesque aliquam maximus risus, vel sed vehicula.</p>
									<p>Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fersapien risus, commodo eget turpis at, elementum convallis elit. Pellentesque enim turpis, hendrerit tristique lorem ipsum dolor.</p> -->

					<hr class="major" />
					<header class="major" id="about">
						<h2>ABOUT THE WORKSHOP</h2>
					</header>
					<p>
						Combating fake news is one of the pressing societal crises. It is difficult to expose false claims before they cause significant damage. Automatic fact and claim verification has recently become a topic of interest among diverse research communities. While research efforts and datasets on text-based fact verification are available, there has been limited attention towards multimodal or cross-modal fact verification. This workshop aims to encourage researchers from interdisciplinary domains working on multimodality and/or fact-checking to come together and work on multimodal (images, memes, videos) fact-checking. At the same time, multimodal hate speech detection is a critical problem but has not received sufficient attention. Lastly, learning joint modalities is of interest to both Natural Language Processing (NLP) and Computer Vision (CV) communities.
					</p>
					<p>
						Over the last decade, both fields of study—NLP and CV—have made significant progress, largely due to the success of neural networks. Multimodal tasks like visual question answering (VQA), image captioning, video captioning, and caption-based image retrieval have gained prominence in both NLP and CV forums. Multimodality is the next big leap for the AI community. De-Factify is a dedicated forum for discussing challenges related to multimodal fake news and hate speech. We also encourage discussions on multimodal tasks in general.

					</p>
					<p>
						Link to previous year workshop : <a href="/2024/index.html" target="_blank"
							rel="noopener noreferrer">Defactify @ AAAI 2024</a>
					</p>

					<h4><a href="#shared">&#9733; Shared Tasks (Yet to be Released)</a></h4>

					<ul>
						<!-- <li>
							Please register here to access the <a
								href="https://codalab.lisn.upsaclay.fr/competitions/16053" target="_blank"
								rel="noopener noreferrer">dataset.</a>
						</li>
						<li>
							Evaluation metric : The official evaluation metric for the shared task is weighted-average
							F1 score.
						</li>
						<li>
							Submission instructions : <strong>TBD</strong>
						</li> -->
						<li>
							System description paper : All teams/participants will be invited to submit a paper
							describing their system. Accepted papers will be published in formal proceedings.
						</li>
						<li>
							Paper submission instruction :<strong>TBD</strong>
						</li>
					</ul>

					<div>
						<div>
							<ol>
								<li>
									<h3> CT<sup>2</sup>: AI-Generated Text Detection</h3>
									<p>
										The widespread use of AI models like ChatGPT has sparked concerns over the proliferation of AI-generated text. Several major organizations and conferences have already banned or restricted its use. The increasing presence of AI-generated content has made it critical to develop reliable methods for detecting such text. Two main approaches have emerged: watermarking, which embeds detectable markers during generation, and post-hoc methods, which analyze and identify AI-generated content after its creation. With the growing complexity of AI systems, the need for effective detection methods has never been more urgent.
									</p>
									<p>
										A new interesting technique proposed at ICML 2024 finds that LLMs are more inclined to modify human-written text than AI-generated text when tasked with rewriting. This approach, named the geneRative AI Detection viA Rewriting method (Raidar), will be used as the baseline for the Shared Tasks.
									  </p>
						  
									  <div style="text-align: center">
										<img
										  src="./images/RAIDAR.png"
										  alt="Raidar Example"
										  style="width: 80%; height: auto; max-width: 600px;"
										/>
										<p>
										  The authors highlight character deletions in red and character insertions in orange. Their findings indicate that human-generated text generally prompts more modifications compared to machine-generated text when rewritten.
										</p>
									  
	
									<section>
								</li>
	
								<li>
									<h3> CT<sup>2</sup>: AI-Generated Image Detection</h3>
	
								
									<p>
										CT<sup>2</sup>: AI-Generated Image Detection addresses the growing challenge of identifying AI-generated images from models like DALL-E, Stable Diffusion, and Midjourney. As generative AI advances, detecting such content is vital for combating misinformation. This project provides a diverse dataset based on MS COCO, showcasing images from various models. Participants must determine whether images are AI-generated and identify the model used. Detection techniques include artifact-based and feature representation methods, enhancing accuracy in identifying AI-generated content.
									</p>
									
									<p>Recent research shows that each generative model leaves detectable traces or "fingerprints" in the frequency domain of images, which can be used for forensic analysis.</p>
									
									<div style="text-align: center" class="intrinsic_section">
										<img
										  src="images/intrinsic_image.png"
										  alt="Artificial Fingerprint Example"
										  class="intrinsic_image"
										/>
										<strong>Detecting Artificial Fingerprints in Generative Model Outputs.</strong>
    									<p>
											Synthetic images generated by models like StyleGAN-T, GALIP, Taming Transformers, DALL·E Mini, Stable Diffusion, and eDiff-I (top) contain unique traces known as artificial fingerprints. These fingerprints are detectable in the frequency domain as spectral peaks in the power spectra (middle) and in the spatial domain as anomalous patterns in the auto-correlation (bottom). Similar artifacts are observed in models sharing similar architectures, enabling forensic analysis of generated content.
	
										</p>
									  </div>
	
									  <p>
										According to a report by the European Union Law Enforcement
										Agency, 90 percent of online content could be synthetically
										generated by 2026. Generative systems like DALL-E and Stable
										Diffusion are impressive but raise concerns about potential
										misuse, particularly in spreading misinformation. Some examples
										can be seen below:
									  </p>
									
									<div class="task2_img_section">
	
										<div class="image-container">
											<div class="image-text">
												<img src="./images/taylor.png" alt="Taylor Swift Image">
												<p>A screenshot from a video showing Taylor Swift holding a flag reading "Trump Won" went viral in February 2024. The video, shared by numerous accounts on X, has reached over 4.5 million viewers. However, it is important to note that this content is AI-generated. For more details, see the <a href="https://www.forbes.com/sites/mattnovak/2024/02/05/viral-video-of-taylor-swift-endorsing-donald-trump-is-completely-fake/" target="_blank">Forbes story</a>.</p>
											</div>
											<div class="image-text">
												<img src="./images/pope.png" alt="Pope Image">
												<p>An AI-generated image of Pope Francis wearing a gigantic white puffer jacket went viral on social media platforms like Reddit and Twitter (X) in March of last year. This image sparked widespread media discussions about the potential misuse of Generative AI technologies, ultimately becoming an iconic example of AI-generated misinformation. For more details, see the <a href="https://www.forbes.com/sites/mattnovak/2023/03/26/that-viral-image-of-pope-francis-wearing-a-white-puffer-coat-is-totally-fake/" target="_blank">Forbes story</a>.</p>
											</div>
										</div>
										
										
	
									</div>
								</li>
							</ol>
	
	
						</div>
					</section>
	



				<section>
					<header class="major" id="call">
						<h2>CALL FOR SUBMISSIONS</h2>
					</header>
					<h3>REGULAR PAPER SUBMISSION</h3>

					<div style="text-align:center">
					</div>

					<h4>Topics of Interests</h4>
					<p>It is a forum to bring attention towards collecting, measuring, managing, mining, and
						understanding multimodal disinformation, misinformation, and malinformation data from social
						media. This workshop covers (but not limited to) the following topics: --</p>
					<ul>
						<li>
							Development of corpora and annotation guidelines for multimodal fact checking.
						</li>
						<li>
							Computational models for multimodal fact checking.
						</li>
						<li>
							Development of corpora and annotation guidelines for multimodal hate speech detection and
							classification.
						</li>
						<li>
							Computational models for multimodal hate speech detection and classification.
						</li>
						<li>
							Analysis of diffusion of Multimodal fake news and hate speech in social networks.
						</li>
						<li>
							Understanding the impact of the hate content on specific groups (like targeted groups).
						</li>
						<li>
							Fake news and hate speech detection in low resourced languages.
						</li>
						<li>
							Hate speech normalization.
						</li>
						<li>
							Case studies and/or surveys related to multimodal fake news or hate speech.
						</li>
						<li>
							Analyzing behavior, psychology of multimodal hate speech/ fake news propagator.
						</li>
						<li>
							Real world/ applied tool development for multimodal hate speech/fake news detection.
						</li>
						<li>
							Early detection of multimodal fake news/hate speech.
						</li>
						<li>
							Use of modalities other than text and images (like audio, video etc).
						</li>
						<li>
							Evolution of multi modal fake news and hate speech.
						</li>
						<li>
							Information extraction, ontology design and knowledge graph for multimodal hate speech and
							fake news.
						</li>
						<li>
							Cross lingual, code-mixed, code switched multimodal fake news/hate speech analysis.
						</li>
						<li>
							Computational social science.
						</li>
					</ul>

					<h4>Submission Instructions:</h4>
					<ul>
						<li><strong>Long papers: </strong> Novel, unpublished, high quality research papers. 10 pages
							excluding references.</li>
						<li><strong>Short papers:</strong> 5 pages excluding references.</li>
						<li><strong>Previously rejected papers:</strong>You can attach comments of previously rejected
							papers (AAAI, neurips) and 1 page cover letter explaining chages made.</li>
						<li><strong>Extended abstracts: </strong>2 pages excluding references. Non archival. can be
							previously published papers or work in progress.</li>
						<li>All papers must be submitted via our EasyChair submission page.</li>
						<li>Regular papers will go through a double-blind peer-review process. Extended abstracts may be
							either single blind (i.e., reviewers are blind, authors have names on submission) or double
							blind (i.e., authors and reviewers are blind). Only manuscripts in PDF or Microsoft Word
							format will be accepted.</li>
						<li>Paper template: <a href="http://ceur-ws.org/Vol-XXX/CEURART.zip" target="_blank"
								rel="noopener noreferrer">http://ceur-ws.org/Vol-XXX/CEURART.zip </a> or <a
								href="https://www.overleaf.com/read/gwhxnqcghhdt" target="_blank"
								rel="noopener noreferrer">https://www.overleaf.com/read/gwhxnqcghhdt</a> </li>
					</ul>

					<p>Paper Submission Link : <a href="https://easychair.org/conferences/?conf=defactify24" target="_blank" rel="noopener noreferrer">EasyChair</a></p>

					<!-- <h4>Important Dates (Round 1):</h4> -->
					<!-- <ul>
										<li> <strong>20 October 2022:</strong> &nbsp; Papers due at 11:59 PM UTC-12</li>
										<li><strong>20 November 2022:</strong> &nbsp;Notification of papers due at 11:59 PM UTC-12</li>
										<li> <strong>10 December 2022:</strong>&nbsp; Camera ready submission due of accepted papers at 11:59 PM UTC-12</li>
										<li> <strong>13-14 February 2024:</strong> &nbsp; Workshop</li>
									</ul> -->
					<!-- <h4>Important Dates (Round 2):</h4> -->
					<h4>Important Dates :</h4>

					<ul>
<!-- 						<li>13 October 2024 : Release of the training set.</li>
						<li>8 November 2024 : Release of the test set.</li>
						<li>30 November 2024 : Deadline for submitting the final results.</li>
						<li>3 December 2024 : Announcement of the results.</li>
						<li>10 December 2024 : System paper submission deadline (All teams are invited to submit a
							paper).</li>
						<li>20 December 2024 : Notification of system papers.</li>
						<li>25 December 2024 : Camera ready submission.</li> -->
						<li>10 January 2025: Papers due at 11:59 PM UTC-12</li>
						<li>20 January 2025: Notification of papers</li>
						<li>25 January 2025: Camera ready submission due of accepted papers at 11:59 PM UTC-12</li>
					</ul>

				</section>

				<section>

					<header class="major" id="shared">
						<h2>Shared tasks</h2>
					</header>

					<!-- <ol>
									 <li> <a href="http://" target="_blank" rel="noopener noreferrer" style="font-weight: 700;"> FACTIFY</a> - Multi-Modal Fact Verification. please visit this link for details.</li>
										<li>  <a href="http://" target="_blank" rel="noopener noreferrer" style="font-weight: 700;"> MEMOTION 2</a>  - Task on analysis of memes. please visit this link for details.</li>
									</ol> -->

					<div class="features" style="text-align: center;">

						<!-- <article> -->

							<!-- <span  style="background-image: url('https://aiisc.ai/defactify/img/factify_logo_nav.png');" ></span> -->
<!-- 

							<div class="content">
								<img src="./images/factifY_4.png" style="width:120px;height: 120px;" alt="" srcset="">
								<p>Factify5WQA - Please visit <a href="./ai_gen_txt_detection.html" target="_blank"
										rel="noopener noreferrer">this link</a> for details.</p>
							</div>



						</article>
						<article>
 -->

							<!-- <div class="content">
								<img src="./images/dehate_logo.png" style="width:120px;height: 120px;" alt="" srcset="">

								<p>DE : HATE - Please visit <a href="./dehate.html" target="_blank"
										rel="noopener noreferrer">this link</a> for details.</p>
							</div> -->


						<!-- </article> -->
						<!-- TODO : ADD REFLINKS TO WEBPAGES. -->
						<article>
							<!-- <span  style="background-image: url('https://aiisc.ai/defactify/img/factify_logo_nav.png');" ></span> -->


							<div class="content">
								<!-- <h3>FACTIFY 2</h3> -->
								<img src="./images/text_detect_shared_task.png" alt="" srcset="">
								<p>Counter Turing Test : Text - Please visit <a href="./ai_gen_txt_detection.html" target="_blank"
										rel="noopener noreferrer">this link</a> for details.</p>
							</div>



						</article>
						<article>
							<!-- <span  style="background-image: url('https://aiisc.ai/defactify/img/factify_logo_nav.png');" ></span> -->


							<div class="content">
								<!-- <h3>FACTIFY 2</h3> -->
								<img src="./images/img_detect_shared_task.png"  alt="" srcset="">
								<p>Counter Turing Test : Images - Please visit <a href="./ai_gen_img_detection.html" target="_blank"
										rel="noopener noreferrer">this link</a> for details.</p>
							</div>



						</article>
						</article>
					</div>

					<h4>Shared Task Important Dates:</h4>
					<ul>
						<li>13 October 2024 : Release of the training set.</li>
						<li>8 November 2024 : Release of the test set.</li>
						<li>30 November 2024 : Deadline for submitting the final results.</li>
						<li>12 December 2024 : Announcement of the results.</li>
						<li>05 January 2025 : System paper submission deadline (All teams are invited to submit a
							paper).</li>
						<li>20 January 2025 : Notification of system papers.</li>
						<li>25 January 2025 : Camera ready submission.</li>
					</ul>
					<!-- <h4>DE : HATE Important Dates:</h4>
					<ul>
						<li> <strong>13 October 2024:</strong> Release of the training set</li>
						<li> <strong>17 November 2024:</strong> Release of the test set</li>
						<li> <strong>23 November 2024:</strong> Deadline for submitting the final results</li>
						<li> <strong>25 November 2024:</strong> Announcement of the results</li>
						<li> <strong>5 December 2024:</strong> System paper submission deadline (All teams are invited
							to submit a paper)</li>
						<li> <strong>12 December 2024:</strong> Notification of system papers</li>
						<li> <strong>23 December 2024:</strong> Camera ready submission</li>
					</ul> -->

				</section>

				<!-- <section>
					<header class="major" id="accepted">
						<h2>Accepted Papers</h2>
					</header>
					<h4>Workshop Regular Papers</h4>
					<ul>
										<li>
											<p>Bias Detection in Text with Dual Transformer Classifier.
												<i>Shaina Raza, Shardul Ghuge, Fatemeh Tavakoli, Sana Ayromlou and Syed Raza Bashir</i></p>
										</li>
										<li>
											<p>Ontology Enhanced Claim Detection.
												<i>Z. Melce Hüsünbeyi and Tatjana Scheffler</i></p>
										</li>
						<li>
											<p>Detecting and Correcting Hate Speech in Multimodal Memes with Large Visual Language Model.
												<i>Minh-Hao Van and Xintao Wu</i></p>
										</li>
					</ul>
											<h4>Factify 5WQA Shared Task Papers</h4>
					<ul>
										<li>
											<p>Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning.
												<i>Shang-Hsuan Chiang, Ming-Chih Lo, Lin-Wei Chao and Wen-Chih Peng</i></p>
										</li>
										<li>
											<p>SRLFactQA at Factify5WQA: Composite Claim-Evidence Consistency Aware Semantic Role Labelling based Question-Answering Entailment.
												<i>Hariram Veeramani, Surendrabikram Thapa, Rajaraman Kanagasabai and Usman Naseem</i></p>
										</li>
						</ul>
											<h4>De:Hate Shared Task Papers</h4>
					<ul>
										<li>
											<p>UniteToModerate at DeHate: The Winning Approach for Segmentation-based Content Moderation with Vision-Text-Mask Modality Fused Large Multimodal Models.
												<i>Hariram Veeramani, Surendrabikram Thapa, Rajaraman Kanagasabai and Usman Naseem</i></p>
										</li>
									</ul>
				</section> -->

				<section>
					<header class="major" id="accepted">
						<h2>Accepted Papers</h2>
					</header>
					<h4>To be announced</h4>
				</section>

				<section>
					<header class="major" id="invited">
						<h2>Invited Talks</h2>

					</header>
					<div>
						<h4>To be announced</h4>
					</div>
				</section>


				<!-- <section>
					<header class="major" id="invited">
					<h2>Invited Talks</h2>
					</header>
					<div style="display: flex;flex-direction:column;gap:30px;">
							
							
						<div style="display: flex;justify-content:space-evenly;text-align: justify;">
								<div style="text-align: center;flex: 1;">
									<img src="https://andreasvlachos.github.io//assets/img/prof_pic.jpg" class="organizer-images" alt="" />
									<h4>Dr. Andreas Vlachos</h4>
									<h4>Department of Computer Science and Technology <br> at the University of Cambridge</h4>
								</div>
								<div>
									<strong>NLP and ML Professor at University of Cambridge.</strong>
								<br>
								<strong>Creator of FEVER Dataset.</strong>
								<br>
								<strong>Organizer of Fake News Challenge.</strong>

								</div>
								
							</div>
							<hr>
							<div style="display: flex;justify-content:space-evenly;text-align: justify;">
								<div style="text-align: center;flex: 1;">
									
									<img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=DfXsKZ4AAAAJ&citpid=3" class="organizer-images" alt="">

										<h4>Dr. Preslav Nakov</h4>
									<h4>Mohamed bin Zayed University of Artificial Intelligence <br> Masdar City, Abu Dhabi</h4>
									</div>
									
								
								<div>	
									<strong>Computer Scientist working on NLP.</strong> <br>
								<strong>Organizer of NLP for Internet Freedom (NLP4IF).</strong> <br>
								<strong>Organizer of OffensEval Task</strong>

								</div>
							
								
					</div>
							
					

					</div>

				</section> -->

				<section>
					<header class="major" id="chairs">
						<h2>
							ORGANIZING COMMITTEE CHAIRS
						</h2>
					</header>

					<div style="display: flex;flex-direction:column;gap:60px">

						<div>
							<h4 style="text-decoration:underline">Dr. Amitava Das:</h4>
							<div>

							<div class="organizer-sections" >
								<div class="image">
									<img src="./images/amitava.jpg"
										class="organizer-images" alt="">
									<br>

								</div>
								<div style="width: 70%;">
									<p>Dr. Amitava Das is a Core Faculty & Research Associate Professor of the
										Artificial Intelligence Institute, at the University of South Carolina, and an
										Advisory Scientist to Wipro AI. <br> <br>
										<strong>Research interests : </strong>Code-Mixing
										and Social Computing.
										<br> <br> <strong>Organizing Activities [selective] : </strong>

										• Memotion @SemEval2020 • SentiMix @SemEval2020 • Computational Approaches to
										Linguistic Code-Switching @LREC 2020 • CONSTRAINT @AAAI2021

									</p>
									<div>
										<b><a href="http://www.amitavadas.com/" target="_blank"
												rel="noopener noreferrer">Web ,</a></b>
										<b><a href="https://scholar.google.co.in/citations?hl=en&user=HYpfhaEAAAAJ"
												target="_blank" rel="noopener noreferrer">Google Scholar,</a></b>
										<b><a href="mailto:amitava.das2@wipro.com" target="_blank"
												rel="noopener noreferrer">Email</a></b>
									</div>


								</div>
							</div>


							</div>
						</div>








						<div>
							<h4 style="text-decoration:underline">Dr. Amit Sheth:</h4>

							<div class="organizer-sections" >
								<div class="image">
									<img src="https://aiisc.ai/defactify/img/committee/amitsheth.jpg"
										class="organizer-images" alt="">
									<br>


								</div>
								<div style="width: 70%;">
									<p>Dr. Amit Sheth is the founding Director of the Artificial Intelligence
										Institute, and a CSE Professor at
										University of South Carolina.
										<br><br>
										<strong>Research interests : </strong>Knowledge
										Graph, NLP, Analysing Social
										Media
										<br><br>
										<strong>Organizing Activities [selective] : </strong>
										• Cysoc2021 @ ICWSM2021 • Emoji2021 @ICWSM2021 • KiLKGC 2021 @KGC21

									<div>
										<b><a href="http://amit.aiisc.ai/" target="_blank" rel="noopener noreferrer">Web
												,</a></b>
										<b><a href="https://scholar.google.com/citations?user=2T3H4ekAAAAJ"
												target="_blank" rel="noopener noreferrer">Google Scholar,</a></b>
										<b><a href="mailto:amit@sc.edu" target="_blank"
												rel="noopener noreferrer">Email</a></b>
									</div>
									</p>


								</div>
							</div>
						</div>

						


						<div>
							<h4 style="text-decoration:underline">Aman Chadha</h4>

							<div class="organizer-sections" >
								<div class="image">
									<img src="./images/aman.jpg" class="organizer-images" alt="">
									<br>
								</div>
								<div style="width: 70%;">
									<p>Aman Chadha is an Applied Sci-
										ence Manager at Amazon Alexa AI
										and a Researcher at Stanford AI.
										<br> <br>
										<strong>Research interests : </strong> Multimodal AI, On-device AI, and
										Human-Centered AI.

										<br> <br>
										<!-- <strong>Organizing Activities [selective] : </strong>  • CONSTRAINT @AAAI2021	 -->
									</p>

									<div>
										<b><a href="https://www.aman.info/" target="_blank"
												rel="noopener noreferrer">Web ,</a></b>
										<b><a href="https://scholar.google.com/citations?user=gPGQuBQAAAAJ&hl=en&oi=ao"
												target="_blank" rel="noopener noreferrer">Google Scholar,</a></b>
										<b><a href="mailto:hi@amanchadha.com" target="_blank"
												rel="noopener noreferrer">Email</a></b>

									</div>


								</div>
							</div>
						</div>

						<div>
							<h4 style="text-decoration:underline">Vasu Sharma</h4>
						
							<div class="organizer-sections" >
								<div class="image">
									<img src="./images/vasu.jpeg" class="organizer-images" alt="Vasu Sharma">
									<br>
								</div>
								<div style="width: 70%;">
									<p>Vasu Sharma is an Applied Research Scientist at FAIR (Meta AI).
										<br> <br>
										<strong>Research interests : </strong> LLMs, Multimodal AI, Data Curation and Efficiency, and Generative AI.
										<br> <br>
									</p>
						
									<div>
										<b><a href="https://bit.ly/47bJZyk" target="_blank" rel="noopener noreferrer">Google Scholar,</a></b>
										<b><a href="mailto:sharma.vasu55@gmail.com" target="_blank" rel="noopener noreferrer">Email</a></b>
									</div>
								</div>
							</div>
						</div>
						
						<div>
							<h4 style="text-decoration:underline">Aishwarya Naresh Reganti</h4>
						
							<div class="organizer-sections" >
								<div class="image">
									<img src="./images/aish.jpeg" class="organizer-images" alt="Aishwarya Naresh Reganti">
									<br>
								</div>
								<div style="width: 70%;">
									<p>Aishwarya Naresh Reganti is an Applied Scientist at AWS Generative AI Innovation Center.
										<br> <br>
										<strong>Research interests : </strong> NLP, Artificial Social Intelligence, Graph Learning.
										<br> <br>
									</p>
						
									<div>
										<b><a href="https://www.linkedin.com/in/areganti/" target="_blank" rel="noopener noreferrer">Web,</a></b>
										<b><a href="https://tinyurl.com/areganti" target="_blank" rel="noopener noreferrer">Google Scholar,</a></b>
										<b><a href="mailto:aish.nr@gmail.com" target="_blank" rel="noopener noreferrer">Email</a></b>
									</div>
								</div>
							</div>
						</div>
						
						<div>
							<h4 style="text-decoration:underline">Vinija Jain</h4>
						
							<div class="organizer-sections" >
								<div class="image">
									<img src="./images/Vinija.jpg" class="organizer-images" alt="Vinija Jain">
									<br>
								</div>
								<div style="width: 70%;">
									<p>Vinija Jain has worked in GenAI, NLP, and RecSys domains at Amazon, Oracle, and PANW.
										<br> <br>
										<strong>Research interests : </strong> NLP, Recommender Systems.
										<br> <br>
									</p>
						
									<div>
										<b><a href="https://vinija.ai/" target="_blank" rel="noopener noreferrer">Web,</a></b>
										<b><a href="https://tinyurl.com/36ncph5y" target="_blank" rel="noopener noreferrer">Google Scholar,</a></b>
										<b><a href="mailto:hi@vinija.ai" target="_blank" rel="noopener noreferrer">Email</a></b>
									</div>
								</div>
							</div>
						</div>
						





					</div>


				</section>

				<section class="organizers" id = "organizers">
					<h2>ASSOCIATE ORGANIZERS</h2>
					<div class="card-container">
						<div class="card">
							<img src="./images/Rajarshi.jpg" alt="Rajarshi Roy" class="organizer-image">
							<h4>Rajarshi Roy</h4>
							<p>Kalyani Government Engineering College</p>
							<div class="links">
								<a href="https://www.linkedin.com/in/rajarshi-roy-learner/" target="_blank">LinkedIn</a>
								<a href="https://rajarshi12321.github.io/rajarshi_portfolio/" target="_blank">Portfolio</a>
							</div>
						</div>

						<div class="card">
							<img src="./images/Shwetangshu.jpeg" alt="Shwetangshu Biswas" class="organizer-image">
							<h4>Shwetangshu Biswas</h4>
							<p>National Institute of Technology Silchar</p>
							<div class="links">
								<a href="https://www.linkedin.com/in/shwetangshu-biswas?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app" target="_blank">LinkedIn</a>
								<a href="https://shwetangshu.vercel.app" target="_blank">Portfolio</a>
								<a href="https://scholar.google.com/citations?user=kBG4NdAAAAAJ&hl=en" target="_blank">Google Scholar</a>
							</div>
						</div>
						<div class="card">
							<img src="./images/Ashhar.jpg" alt="Ashhar Aziz" class="organizer-image">
							<h4>Ashhar Aziz</h4>
							<p>IIIT Delhi</p>
							<div class="links">
								<a href="https://www.linkedin.com/in/ashhar101/" target="_blank">LinkedIn</a>
								<a href="https://scholar.google.com/citations?user=uHIsCbsAAAAJ&hl=en" target="_blank">Google Scholar</a>
							</div>
						</div>
						<div class="card">
							<img src="./images/Shreyas.jpeg" alt="Shreyas Dixit" class="organizer-image">
							<h4>Shreyas Dixit</h4>
							<p>Vishwakarma Institute of Information Technology</p>
							<div class="links">
								<a href="https://www.linkedin.com/in/srddev/" target="_blank">LinkedIn</a>
								<a href="mailto:shreyasrd31@gmail.com" target="_blank">Email</a>
								<a href="https://scholar.google.com/citations?user=pl_o-VUAAAAJ&hl=ena" target="_blank">Google Scholar</a>
							</div>
						</div>
						<div class="card">
							<img src="https://parthpatwa.github.io/assets/parth.jpg" alt="Subhankar Ghosh" class="organizer-image">
							<h4>Parth Patwa</h4>
							<p>University of California Los Angeles (UCLA)</p>
							<div class="links">
								<a href="https://parthpatwa.github.io/" target="_blank" rel="noopener noreferrer">Web ,</a>
								<a href="mailto:parthpatwa@g.ucla.edu" target="_blank" rel="noopener noreferrer">Email</a>
								<a href="https://scholar.google.com/citations?user=nlpQCpsAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Google Scholar,</a>
							
							</div>
						</div>
						<div class="card">
							<img src="./images/Subhankar.jpeg" alt="Subhankar Ghosh" class="organizer-image">
							<h4>Subhankar Ghosh</h4>
							<p>Washington State University</p>
							<div class="links">
								<a href="https://www.linkedin.com/in/subhankar-ghosh-11701a169/" target="_blank">LinkedIn</a>
								<a href="https://subhankarghoshss.github.io/website/" target="_blank">Portfolio</a>
								<a href="https://scholar.google.com/citations?user=1Q73N6IAAAAJ&hl=en" target="_blank">Google Scholar</a>
							</div>
						</div>
						<div class="card">
							<img src="./images/Shashwat.jpg" alt="Shashwat Bajpai" class="organizer-image">
							<h4>Shashwat Bajpai</h4>
							<p>BITS Pilani Hyderabad Campus</p>
							<div class="links">
								<a href="https://www.linkedin.com/in/sbajpai1729/" target="_blank">LinkedIn</a>
								<a href="https://shashwat1729.netlify.app/" target="_blank">Portfolio</a>
								<a href="https://scholar.google.com/citations?user=0CBkKGYAAAAJ&hl=en" target="_blank">Google Scholar</a>
							</div>
						</div>
						<div class="card">
							<img src="./images/Nilesh.jpg" alt="Shreyas Dixit" class="organizer-image">
							<h4>Nilesh Ranjan Pal</h4>
							<p> Kalyani Government Engineering College</p>
							<div class="links">
								<a href="https://www.linkedin.com/in/nilesh-ranjan-pal/" target="_blank">LinkedIn</a>
								<!-- <a href="mailto:shreyasrd31@gmail.com" target="_blank">Email</a> -->
								<a href="https://smallboy713102.github.io/nilesh.github.io/" target="_blank">Portfolio</a>
							</div>
						</div>
						<div class="card">
							<img src="./images/Kapil.jpg" alt="Kapil Wanaskar" class="organizer-image">
							<h4>Kapil Wanaskar</h4>
							<p>San José State University, California, USA</p>
							<div class="links">
								<a href="https://www.linkedin.com/in/kapil-wanaskar-06507483/" target="_blank">LinkedIn</a>
								<a href="https://scholar.google.com/citations?user=fDo5HioAAAAJ&hl=en" target="_blank">Google Scholar</a>
							</div>
						</div>
						<div class="card">
							<img src="./images/Nasrin.jpg" alt="Nasrin Imanpour" class="organizer-image">
							<h4>Nasrin Imanpour</h4>
							<p>University of South Carolina</p>
							<div class="links">
								<a href="https://www.linkedin.com/in/nasrin-imanpour-9212b0229" target="_blank">LinkedIn</a>
								<a href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=MA-AiBIAAAAJ&sortby=pubdate" target="_blank">Google Scholar</a>
							</div>
						</div>
						
						
						
					</div>
				
				</section>
				

				<section>
					<header class="major" id="contact">
						<h2>CONTACT US</h2>
					</header>

					<ul class="contact">
						<li class="icon solid fa-envelope"><a href="mailto:amitava.santu@gmail.com">
								amitava.santu@gmail.com</a></li>
						<li class="icon solid fa-envelope"><a href="mailto:parthpatwa@ucla.edu"> parthpatwa@ucla.edu</a>
						</li>
						<li class="icon solid fa-envelope"><a href="mailto:defactifyaaai@gmail.com
							">defactifyaaai@gmail.com
							</a>
						</li>

					</ul>
				</section>

			</div>
		</div>

		<!-- Sidebar -->
		<div id="sidebar">
			<div class="inner">

				<!-- Search -->
				<!-- <section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section> -->

				<!-- Menu -->
				<nav id="menu">
					<header class="major">
						<h2>Menu</h2>
					</header>
					<ul>
						<li><a href="#about">ABOUT THE WORKSHOP</a></li>
						<li><a href="#call">CALL FOR SUBMISSIONS</a></li>

						<li><a href="#shared">SHARED TASKS</a></li>


<!-- 						<li><a href="#awards">AWARDS</a></li> -->
						<li><a href="#accepted">ACCEPTED PAPERS</a></li>

						<li><a href="#invited">INVITED TALKS</a></li>
						<li><a href="#chairs">ORGANIZING COMMITTEE CHAIRS</a></li>
						<li><a href="#organizers">ASSOCIATE ORGANIZERS</a></li>
						<li><a href="#contact">CONTACT US</a></li>

					</ul>
					<hr>
					<p>
						Link to previous year workshop : <a href="/2024/index.html" target="_blank"
							rel="noopener noreferrer">Defactify @ AAAI 2024</a>
					</p>

					<br>
					<br>
					<a href="ai_gen_txt_detection.html" style="font-size: 13px;">
						CT2 : AI-Generated Text Detection
					</a>
					<br>
					<a href="ai_gen_img_detection.html" style="font-size: 13px;">
						CT2 : AI-Generated Image Detection
					</a>

				</nav>



			</div>
		</div>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>
